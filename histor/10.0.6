

-----------------------------------------------------------------------

--------------------------------------------------------------------------------
--- AUTEUR meunier      MEUNIER Sébastien      DATE 15/06/2009 - 15:16:39

--------------------------------------------------------------------------------
RESTITUTION FICHE 013495 DU 2009-06-10 08:56:08
TYPE express concernant Code_Aster (VERSION 10.1)
TITRE
   ARLEQUIN - Modifications suite xc3xa0 restitution 11717
FONCTIONNALITE
   Suite à la restitution de la fiche 11717 effectuée par Josselin, je me place responsable
   sur le source suivant :
   
   fortran :
   --------
   arlapl.f  arlteb.f  arlted.f  arlten.f  bmatf1.f  te0119.f
   arltds.f  arltec.f  arltem.f  arltep.f  bmatfr.f
   
   option :
   --------
   arlq_matr.cata
   
   Suite à la réorganisation des tests ssnp135* (voir fiche 12952), je supprime les fichiers
   superflus suivants, correspondants aux anciens maillages gmsh ou gibi :
   
   ssnp135b.38 ssnp135b.37 ssnp135b.22 ssnp135b.19
   ssnp135c.38 ssnp135c.37 ssnp135c.22 ssnp135c.19
   ssnp135e.38 ssnp135e.37 ssnp135e.22 ssnp135e.19
RESU_FAUX_VERSION_EXPLOITATION    :  NON
RESU_FAUX_VERSION_DEVELOPPEMENT   :  NON
RESTITUTION_VERSION_EXPLOITATION  :  NON
RESTITUTION_VERSION_DEVELOPPEMENT :  OUI
IMPACT_DOCUMENTAIRE : 
VALIDATION
   Aucune
NB_JOURS_TRAV  : 0.1
--------------------------------------------------------------------------------



-----------------------------------------------------------------------

--------------------------------------------------------------------------------
--- AUTEUR pellet       PELLET Jacques         DATE 16/06/2009 - 15:39:24

--------------------------------------------------------------------------------
RESTITUTION FICHE 011392 DU 2007-10-23 16:53:31
TYPE evolution concernant Code_Aster (VERSION )
TITRE
   SV10 -  Parallxc3xa9lisme des calculs xc3xa9lxc3xa9mentaires. Partitionnement en sous-domaines adaptxc3xa9s pour MUMPS distribuxc3xa9
FONCTIONNALITE
   Evolution souhaitée :
   ---------------------
   Acuellement, les calculs élémentaires ne sont parallélisés que si le solveur associé à la
   commande (s'il existe) est le solveur "Mumps distribué" (ou bien FETI).
                                                                                            
                           
   On souhaite 2 choses :
   1) Que les calculs élémentaires puissent etre parallélisés (i.e. distribués) qu'il y ait
   un solveur ou non dans la commande.
   2) Que ce parallélisme soit activé par défaut pour la version mpi d'Aster.
      En d'autres termes, on souhaite qu'un calcul puisse etre lancé en parallèle (et tirer
   parti du parallélisme !) sans que l'on soit obligé de modifier son fichier de commandes :
        - les calculs élémentaires seront toujours parallélisés
        - l'assemblage des matrices élémentaires sera toujours parallélisé
        - le solveur sera "parallèle" si l'utilisateur a choisi  'MUMPS' ou 'PETSC'
                                                                                            
                                                                                            
                       
                           
   Principe de la mise en oeuvre du parallélisme des calculs élémentaires :
   ------------------------------------------------------------------------
   Les calculs élémentaires qui seront "distribués" entre les différents processeurs sont
   ceux qui concernent les éléments du modèle (dont la maille support appartient au
   maillage). Les éléments "tardifs" (dualisation des C.L., condition d'échange en thermique,
   ...) ne sont pas parallélisés.
                                                                                            
                           
   La répartition des mailles du maillage entre les différents processeurs peut etre pilotée
   par l'utilisateur. S'il ne dit rien, on effectue une répartition "équitable" en
   distribuant les mailles aux N processeurs comme on distribuerait les cartes entre N
   joueurs de poker (PARALLELISME='MAIL_DISPERSE').
                                                                                            
                           
   L'objet contenant cette répartition est une nouvelle structure de données (sd_partition).
   Elle est calculée dans AFFE_MODELE et stockée dans la sd_modele.
                                                                                            
                           
   Avantage de cette solution :
     - La partition étant associée au modèle, elle est toujours accessible lorsque l'on fait
   des calculs élémentaires sur le modèle.
     - Il n'y a donc pas de nouvel argument (la partition) à faire transiter entre toutes les
   routines d'une commande.
     - Cela évite aussi d'ajouter un mot clé PARTITION dans de nombreuses commandes.
                                                                                            
                           
   Inconvénient :
     - La partition étant stockée dans la sd_modele (base globale), quand on est en
   poursuite, il faudrait normalement poursuivre le calcul sur le meme nombre de processeurs.
   Ce qui n'est pas forcément souhaitable.
     - Pour contourner cette difficulté, nous proposons une nouvelle commande MODI_MODELE qui
   ne sert qu'à modifier la partition du modèle.
     - Il est donc conseillé de commencer les fichiers de commande de type "POURSUITE" par :
       MODI_MODELE(reuse=MO, MODELE=MO)
       Cette commande créera une nouvelle partition adaptée au nombre de processeurs disponibles.
                                                                                            
                           
   Syntaxe :
   -------------
                                                                                            
                           
   AFFE_MODELE /MODI_MODELE : ajout du mot clé facteur (facultatif) PARTTION :
                                                                                            
                           
            PARTITION         =FACT(statut='d',
                PARALLELISME        =SIMP(statut='f',typ='TXM',defaut="MAIL_DISPERSE",
                                     
   into=("MAIL_CONTIGU","MAIL_DISPERSE","SOUS_DOMAINE","CENTRALISE",)),
                b_dist_maille       =BLOC(condition = "PARALLELISME in
   ('MAIL_DISPERSE','MAIL_CONTIGU')",
                    CHARGE_PROC0_MA =SIMP(statut='f',typ='I',defaut=100,val_min=0),
                ),
                b_dist_sd           =BLOC(condition = "PARALLELISME == 'SOUS_DOMAINE'",
                    PARTITION       =SIMP(statut='o',typ=sd_feti_sdaster),
                    CHARGE_PROC0_SD =SIMP(statut='f',typ='I',defaut=0,val_min=0),
                ),
            ),
                                                                                            
                           
   Modification de tous les catalogues des commandes qui utilisaient le mot clé SOLVEUR / MUMPS :
   244,248d243
   <              PARALLELISME    =SIMP(statut='f',typ='TXM',defaut="CENTRALISE",
   <                                   
   into=("CENTRALISE","MAIL_CONTIGU","MAIL_DISPERSE","SOUS_DOMAINE")),
   <              PARTITION       =SIMP(statut='f',typ=sd_feti_sdaster),
   <              CHARGE_PROC0_MA =SIMP(statut='f',typ='I',defaut=100,val_min=0),
   <              CHARGE_PROC0_SD =SIMP(statut='f',typ='I',defaut=0,val_min=0),
                                                                                            
                           
   Ces modifications changent donc l'usage du solveur MUMPS :
                                                                                            
                           Avant..............................................Après
   -----------------------------------------------------------------------------------------------------------------
   ...................................................MO=AFFE_MODELE(PARTITION=_F(PARALLELISME='SOUS_DOMAINE',...)
   SOLVEUR=_F(METHODE='MUMPS',........................SOLVEUR=_F(METHODE='MUMPS'),
   ...........PARALLELISME='DISTRIBUE_SD',........->
   -----------------------------------------------------------------------------------------------------------------
   ...................................................MO=AFFE_MODELE(PARTITION=_F(PARALLELISME='MAIL_CONTIGU',...)
   SOLVEUR=_F(METHODE='MUMPS',........................SOLVEUR=_F(METHODE='MUMPS'),
   ...........PARALLELISME='DISTRIBUE_MC',........->
   -----------------------------------------------------------------------------------------------------------------
   ...................................................MO=AFFE_MODELE(PARTITION=_F(PARALLELISME='MAIL_DISPERSE',...)
   SOLVEUR=_F(METHODE='MUMPS',........................SOLVEUR=_F(METHODE='MUMPS'),
   ...........PARALLELISME='DISTRIBUE_MD',........->
   -----------------------------------------------------------------------------------------------------------------
   ...................................................MO=AFFE_MODELE(PARTITION=_F(PARALLELISME='NON',...)
   SOLVEUR=_F(METHODE='MUMPS',........................SOLVEUR=_F(METHODE='MUMPS'),
   ...........PARALLELISME='CENTRALISE',........->
   -----------------------------------------------------------------------------------------------------------------
   
                                                                                            
                           
   Modifs SD :
   ------------
   Une nouvelle structure de données est proposée : la sd_partition (K8).
   sd_partition (K8) :: record
      .NUPROC.MAILLE  V I long=nb_mailles(maillage) :
          V(ima) : numéro du processeur qui calcule l'élément fini porté par la maille ima du
   maillage.
                                                                                            
                           
   Remarque : les éléments s'appuyant sur des mailles tardives sont tous calculés par le
   processeur 0.
                                                                                            
                           
   sd_ligrel :
      .LGRF(2) -> sd_partition
   sd_solveur :
      .SLVK(7) -> ' '
                                                                                            
                           
                                                                                            
                           
   Détails :
   ----------
   *) Suppression du COMMON CADII18 (CADIST)
      Ce common assurait que les calculs élémentaires distribués étaient bien assemblés en
   parallèle.
   *) On assouplit celces.f pour qu'elle complète le cham_elem "in" si nécessaire.
      Cette évolution est nécessaire pour que les commandes DEFI_FISS_XFEM, MODI_FISS_XFEM,
   ...  fonctionnent encore en mode //.
                                                                                            
                           
                                                                                            
                           
   Validation (en parallèle sur 2 processeurs):
   ----------------------------------------------
   *) Passage de 21 tests (sur 23) de la liste des tests MPI (tous sauf mumps05b qui est très
   long)
   *) Passage d'un test factoriser + calc_cham_elem : tplv100c
   *) Passage d'un test defi_fiss_xfem : ssnv173i
   *) Passage d'un test calc_elem +  OPTION=('ERZ1_ELEM_SIGM','SING_ELEM','SING_ELNO_ELEM') :
   ssnp130c
   *) Passage d'un test post_elem + integrale + group_ma : sslv140a
   *) Passage d'un test macr_elem_stat : sdls01d
   *) Passage de 100 tests en // :
     ssll107c  tpll01i   tpll01d   tplp303f  tpla07a   tpll01f   tpla06a
     sdll11i   tpnv01b   tpnl300b  sslp301a  tpla01c   sdls502d  tpna300b
     tpla01b   zzzz167b  tpll01e   feti006a  fdll200a  tpla04a   sensm02a
     zzzz176a  sdls502b  ssla100c  sdls502a  ahlv302f  ssla100b  sdls503b
     hplp300a  yyyy100z  tplv304a  tplp301a  hpla101a  tpna300d  zzzz140a
     tplp303a  zzzz229a  sslp107a  zzzz228a  zzzz167c  sdls503a  sdls502c
     zzzz233a  ttll301b  forma06a  feti003b  zzzz164a  sensm06a  sdll11f
     zzzz164b  sdld400b  erreu01a  feti003c  ssll107e  sensm06b  tplv305b
     feti005a  ssls503c  feti003a  tplp303d  tplp303b  feti004a  szlz102a
     ahlv302e  ssls122b  ahlv100m  tpla06b   sdls503c  tpna300a  ssls122c
     ssls122a  zzzz100e  szlz100a  zzzz165a  tplp300a  zzzz125a  feti002a
     tpla05b   forma03a  sdlv401a  feti001a  tplp01a   szlz108a  tpla300a
     szlz106a  zzzz167a  szlz111a  tplp301b  szlz111b  tplp303e  szlz103a
     szlz101a  szlz109a  zzzz206a  rccm09a   tpla301a  szlz107a  zzzz128b
     zzzz102b  zzzz103a
                                                                                            
                           
                                                                                            
                           
   Limitation pour les calculs parallèles :
   ----------------------------------------
    Volumes finis :
   ------------------
     La logique de la distribution des calculs élémentaires est incompatible avec la notion
   de "voisinage" qui est utilisée pour les volumes finis.
     Un calcul élémentaire sur un volume fini ne sera juste que si tous ses voisins sont
   aussi traités par le meme processeur. De proche en proche, on voit que tous les élements
   doivent etre traités par le meme processeur !
                                                                                            
                           
                                                                                            
                           
   Performances:
   -------------
   Pour évaluer l'efficacité du parallélisme des calculs élémentaires, nous avons fait le
   test suivant :
                                                                                            
                           
   * Maillage : 500 000 mailles QUAD4
   * CALC_ELEM / SIEF_ELGA_DEPL sur un seul pas de temps
   * mise en oeuvre du parallélisme sur la seule commande CALC_ELEM (en POURSUITE)
                                                                                            
                           
   Nous avons lancé ce test en séquentiel, puis sur 2, 4, 8 et 16 processeurs.
   Le temps mesuré est le temps "elapsed".
   Nous avons pris comme temps elapsed pour 1 calcul le "max" des temps elapsed des
   différents processeurs.
                                                                                            
                           
   Chaque calcul a été lancé 3 fois afin de mesurer la variabilité de la mesure.
                                                                                            
                           
   Les résultats sont :
                                                                                            
                           
   nproc   elapsed      elapsed
            min          max
   1       51,15        51,44
   2       27,50        28,44
   4       16,63        17,50
   8       11,46        12,24
   16      9,38         10,35
                                                                                            
                           
   La dispersion des résultats est acceptable mais nous n'avons aucune explication pour cette
   variation : les calculs ont tous été lancés sur le meme noeud dédié à nos essais (aster8).
                                                                                            
                           
   Dans le tableau suivant (qui mesure l'efficacité du parallélisme), nous avons choisit le
   temps "min" c'est à dire la performance que la machine est capable de produire quand elle
   le veut bien ...
                                                                                            
                           
   nproc   elapsed   speed-up     efficacité //
   1       51,15     1,00          1,00
   2       27,50     1,86          0,93
   4       16,63     3,08          0,77
   8       11,46     4,46          0,56
   16      9,38      5,45          0,34
                                                                                            
                           
                                                                                            
                           
   Conclusions :
   -------------
   1) Nous obtenons un speed-up de 3 sur 4 processeurs, mais ce speed-up semble avoir du mal
   à dépasser les 6.
      Il est donc conseillé de se limiter à 4 processeurs.
   2) Ce cas test n'est sans doute pas le plus favorable au parallélisme :
      - le calcul de SIEF_ELGA_DEPL est "simple" et ne nécessite pas beaucoup de temps de calcul.
      - les échanges MPI après les calculs élémentaires sont obligatoires pour que les champs
   calculés dans CALC_ELEM soient "complets" sur toutes les bases globales.
      - Il serait intéressant de mesurer l'efficacité du parallélisme sur des calculs plus
   couteux (FULL_MECA avec un comportement "difficile"). D'autant plus que dans ce cas, les
   échanges MPI sont moins nombreux en proportion : les seuls champs à "compléter" sont les
   champs SIEF_ELGA et VARI_ELGA pour les instants d'archivage.
                                                                                            
                           
                                                                                            
                           
   Impact documentaire :
   ---------------------
   U4.41.01 : AFFE_MODELE
   U4.41.?? : MODI_MODELE
   U4.50.01 : mot clé SOLVEUR
                                                                                            
                           
   Nb_jours_trav :
   ---------------
   JP : 4
RESU_FAUX_VERSION_EXPLOITATION    :  NON
RESU_FAUX_VERSION_DEVELOPPEMENT   :  NON
RESTITUTION_VERSION_EXPLOITATION  :  NON
RESTITUTION_VERSION_DEVELOPPEMENT :  OUI
IMPACT_DOCUMENTAIRE : U4.41.01 U4.41.02 U4.50.01
VALIDATION
   plus de 100 tests en parallxc3xa8le
NB_JOURS_TRAV  : 4.0
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
--- AUTEUR lefebvre     LEFEBVRE Jean-Pierre   DATE 16/06/2009 - 15:39:25

--------------------------------------------------------------------------------
RESTITUTION FICHE 013499 DU 2009-06-11 05:51:56
TYPE express concernant Code_Aster (VERSION 7.0)
TITRE
   Passage xc3xa0 la librairie MUMPS 4.8.4
FONCTIONNALITE
   Suite aux développements réalisés par O.Boiteau en version 10.0.5, il est maintenant
   possible de basculer sur les librairies MUMPS 4.8.4. 
   Pour cela, il faut : 
      - modifier les fichiers de configuration : config.txt, config.mpi, config.zmat et
   config.prof,
      - recompiler tous les fichiers du répertoire bibf90/mumps pour utiliser les includes
   Fortran90.
   
   La restitution est effectuée avec le fichier de configuration modifié. Les nouveaux
   fichiers de configuration seront basculés juste avant la mise à jour, après  la fin de
   tous les arest des développeurs.
RESU_FAUX_VERSION_EXPLOITATION    :  NON
RESU_FAUX_VERSION_DEVELOPPEMENT   :  NON
RESTITUTION_VERSION_EXPLOITATION  :  NON
RESTITUTION_VERSION_DEVELOPPEMENT :  OUI
IMPACT_DOCUMENTAIRE : 
VALIDATION
   tests utilisant le solveur MUMPS
NB_JOURS_TRAV  : 0.5
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
--- AUTEUR pellet       PELLET Jacques         DATE 16/06/2009 - 15:39:25

--------------------------------------------------------------------------------
RESTITUTION FICHE 013501 DU 2009-06-11 07:53:11
TYPE evolution concernant Code_Aster (VERSION )
TITRE
   write(6,*) dans assvec
FONCTIONNALITE
   Problème :
   ----------
   Lors de am dernière restitution, j'ai laissé trainer un "WRITE(6,*)" de debug dans la
   routine assvec.f
   
   Correction :
   -----------
   Je retire la ligne coupable.
RESU_FAUX_VERSION_EXPLOITATION    :  NON
RESU_FAUX_VERSION_DEVELOPPEMENT   :  NON
RESTITUTION_VERSION_EXPLOITATION  :  NON
RESTITUTION_VERSION_DEVELOPPEMENT :  OUI
IMPACT_DOCUMENTAIRE : 
VALIDATION
   rien departiculier
NB_JOURS_TRAV  : 0.05
--------------------------------------------------------------------------------
RESTITUTION FICHE 013475 DU 2009-06-04 08:13:29
TYPE express concernant Code_Aster (VERSION 7.0)
TITRE
   parallelisme='DISTRIBUE_MC' pas effcace du tout !
FONCTIONNALITE
   Problème:
   ---------
   Pour le solveur "mumps distribué", la répartition des mailles entres les différents
   processeurs
     PARALLELISME= / 'DISTRIBUE_MC'
                   / 'DISTRIBUE_MD'
   est mal faite.
                                                                                            
                           
   Correction:
   -----------
   Il y a 3 coquilles dans la routine crsvmu.f dont les conséquences sont :
                                                                                            
                           
     'DISTRIBUE_MC' :
           -> Toutes les mailles sont affectées au processeur 0 !!!
     'DISTRIBUE_MD' :
           -> le monitoring est erroné (nombre de mailles par proc)
           -> le mot clé CHARGE_PROC0_MA est sans influence (i.e. la charge est répartie
   équitablement entre les processeurs).
                                                                                            
                           
   Resultats FAUX ?
   -----------------
   Non. Le problème ne concerne que la répartition de la charge de calcul entre les processeurs.
                                                                                            
                           
                                                                                            
                           
   Validation :
   ------------
   Quelques essais perso pour vérifier que les mailles se répartissent comme elles le doivent
   entre 4 processeurs.
   J'ai également vérifié le role du mot cle CHARGE_PROC0_MA.
                                                                                            
                           
   Version NEW10 :
   --------------
   On ne corrige rien dans le cadre de cette fiche car elle va etre modifiée dans le cadre
   des fiches 11392 et 13139.
                                                                                            
                           
   Version NEW9 :
   --------------
   Les modifications à reporter dans crsvmu.f en NEW9 sont :
   203,204c203
   <           NMP0=MAX(NBMAMO,(DIST0*NMPP)/100)
   ---
   >           NMP0=(DIST0*NMPP)/100
   218c215
   <             IF (PLEIN0) RANG=RANG+1
   ---
   >             IF (PLEIN0) KRANG=KRANG+1
   233c230
   <           NMP0=MAX(NBMAMO,(DIST0*NMPP)/100)
   ---
   >           NMP0=(DIST0*NMPP)/100
RESU_FAUX_VERSION_EXPLOITATION    :  NON
RESU_FAUX_VERSION_DEVELOPPEMENT   :  NON
RESTITUTION_VERSION_EXPLOITATION  :  OUI
RESTITUTION_VERSION_DEVELOPPEMENT :  OUI
IMPACT_DOCUMENTAIRE : 
VALIDATION
   essais perso
NB_JOURS_TRAV  : 0.3
--------------------------------------------------------------------------------
RESTITUTION FICHE 013483 DU 2009-06-08 07:30:59
TYPE express concernant Code_Aster (VERSION 7.0)
TITRE
   un bug dans la routine detgnm.f
FONCTIONNALITE
   Problème:
   ---------
   Jean-Michel a mis en évidence un problème concernant la commande DEFI_GROUP/DETR_GROUP_NO.
   Dans une boucle de destruction de groupes de noeuds, il est arreté par un message du type :
   Le répertoire de longueur 38 ne contient pas 39.
                                                                                            
                                                                  
   Analyse :
   ---------
   Le problème vient de la routine detgnm.f :
     * On note le nombre de groupes avant la destuction  :  NBTGP
     * On calcule le nombre de groupes à détruire        :  NBGMDE
     * On crée une nouvelle collection .GROUPNO (ou .GROUPMA) de dimension (NBTGP - NBGMDE)
     * puis on recopie les groupes qui n'ont pas été détruits.
                                                                                            
                                                                  
   Malheureusement, NBTGP peut etre parfois sur-dimensonné car il est obtenu par NMAXOC (au
   lieu de NUTIOC).
   Du coup, on est arreté lorsque l'on cherche à recopier un groupe qui n'existe pas.
                                                                                            
                                                                  
   Ce qui est étonnant, c'est que ce problème ne se soit pas manifesté plus rapidement : le
   bug existe depuis la version 8.1.6.
   Il faut croire que le "sur-dimensionnement" des collections .GROUPENO et .GROUPEMA n'est
   pas si fréquent que cela.
    
                                                                                            
                                                                  
   Correction :
   ------------
   On corrige la routine detgnm.f :
   65c65
   <             CALL JELIRA(MA//GROUP(IG),'NMAXOC',NBTGP,KBID)
   ---
   >             CALL JELIRA(MA//GROUP(IG),'NUTIOC',NBTGP,KBID)
                                                                                            
                                                                  
   NEW9 :
   ------
   reporter la correction ci-dessus en version 9
                                                                                            
                                                                  
   NB_JOURS_TRAV : 0.55 (JMP:0.5, JP=0.05)
RESU_FAUX_VERSION_EXPLOITATION    :  NON
RESU_FAUX_VERSION_DEVELOPPEMENT   :  NON
RESTITUTION_VERSION_EXPLOITATION  :  OUI
RESTITUTION_VERSION_DEVELOPPEMENT :  OUI
IMPACT_DOCUMENTAIRE : 
VALIDATION
   JMP a testxc3xa9 la correction
NB_JOURS_TRAV  : 0.55
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
--- AUTEUR sfayolle     FAYOLLE Sebastien      DATE 16/06/2009 - 15:39:26

--------------------------------------------------------------------------------
RESTITUTION FICHE 013497 DU 2009-06-10 14:49:42
TYPE express concernant Code_Aster (VERSION 7.0)
TITRE
   26 tests cassxc3xa9s sur bull en 10.0.5 : fdlv106a, sdld21a, ...
FONCTIONNALITE
   Problème
   -----------
   
   97 cas tests sont cassés en 10.0.5
   
   Corrections
   -------------
   
   Deux erreurs ont été identifiées et corrigées dans meamme.f et dans te0050.f
   
   Validations :
   --------------
   
   cas test cassés
   
   P.S.
   -----
   
   39 cas tests restent cassés mais ne sont pas du à ma restitution
   
   ascou03a     <S>_ERROR
   ascou12a     <S>_ERROR
   fiab001a     <S>_ERROR
   sdll113b     NOOK_TEST_RESU
   sdll113c     NOOK_TEST_RESU
   sdll130a     NOOK_TEST_RESU
   sdll130b     NOOK_TEST_RESU
   sdls01b      <F>_ERROR
   sdls08a      <F>_ERROR
   sdls114b     <F>_ERROR
   sdls115a     <F>_ERROR
   sdls119a     <F>_ERROR
   sdlv120a     <F>_ERROR
   sdlv124a     <F>_ERROR
   sdlv302c     <F>_ERROR
   sdlv302d     <F>_ERROR
   sdlx100a     <F>_ERROR
   sdnl111c     <F>_ERROR
   sdnl130a     NOOK_TEST_RESU
   sdns106e     <F>_ERROR
   sensd02a     <F>_ERROR
   sensd05a     <F>_ERROR
   sensd07a     <F>_ERROR
   sensd08a     <F>_ERROR
   sensd08b     <F>_ERROR
   sensd10d     <F>_ERROR
   shll101d     <F>_ERROR
   shll101e     <F>_ERROR
   shll101f     <F>_ERROR
   shlv301a     <F>_ERROR
   sslp104b     <F>_ERROR
   sslv110f     <S>_ERROR
   sslv134d     NOOK_TEST_RESU
   ssnv185f     <S>_CPU_LIMIT
   ssnv503b     <F>_ERROR
   zzzz159f     <F>_ERROR
   aspic09a     <F>_ERROR
   miss06b      <F>_ERROR
   miss06c      <F>_ERROR
RESU_FAUX_VERSION_EXPLOITATION    :  NON
RESU_FAUX_VERSION_DEVELOPPEMENT   :  NON
RESTITUTION_VERSION_EXPLOITATION  :  NON
RESTITUTION_VERSION_DEVELOPPEMENT :  OUI
IMPACT_DOCUMENTAIRE : 
VALIDATION
   cas tests casses
NB_JOURS_TRAV  : 0.5
--------------------------------------------------------------------------------



========================================================================
=== Recapitulation des operations demandees pour toutes les restitutions
========================================================================


    TYPE Action    unite                      user      Auteur         nblg  ajout suppr.

 CASTEST MODIF mumps01a                       pellet J.PELLET           160     23     21
 CASTEST MODIF mumps02a                       pellet J.PELLET           110      8      9
 CASTEST MODIF mumps04a                       pellet J.PELLET           362     26     24
 CASTEST MODIF mumps05a                       pellet J.PELLET           288      9     12
 CASTEST MODIF mumps05b                       pellet J.PELLET           161      5     13
 CASTEST MODIF ssll106f                       pellet J.PELLET           581     16      9
 CASTEST MODIF ssll117f                       pellet J.PELLET           201      2      2
 CASTEST MODIF ssls09c                        pellet J.PELLET           198      2      2
 CASTEST MODIF ssna117b                       pellet J.PELLET           186      2      2
 CASTEST MODIF ssnl101a                       pellet J.PELLET           112      2      2
 CASTEST MODIF ssnl125a                       pellet J.PELLET           266      2      2
 CASTEST MODIF ssnp118a                       pellet J.PELLET           395      5      5
 CASTEST MODIF ssnp118d                       pellet J.PELLET           387      5      5
 CASTEST MODIF ssnp121l                       pellet J.PELLET           172      2      2
 CASTEST MODIF ssnp124b                       pellet J.PELLET           291      2      2
 CASTEST MODIF ssnv112d                       pellet J.PELLET           302      2      2
 CASTEST MODIF ssnv126a                       pellet J.PELLET           462      2      2
 CASTEST MODIF ssnv129e                       pellet J.PELLET           209      2      2
 CASTEST MODIF ssnv173i                       pellet J.PELLET           234      6     12
 CASTEST MODIF wtna101a                       pellet J.PELLET           368      3      3
 CASTEST MODIF wtna108a                       pellet J.PELLET           361      2      2
 CASTEST MODIF wtna108b                       pellet J.PELLET           361      2      2
 CASTEST MODIF wtna108c                       pellet J.PELLET           361      2      2
 CASTEST MODIF wtnp109a                       pellet J.PELLET           360      2      2
 CASTEST MODIF wtnp111b                       pellet J.PELLET           173      2      5
 CASTEST MODIF wtnv100c                       pellet J.PELLET           443      3      3
CATALOGU MODIF options/arlq_matr             meunier S.MEUNIER           43      2      1
CATALOPY AJOUT commande/modi_modele           pellet J.PELLET            38     38      0
CATALOPY MODIF commande/affe_modele           pellet J.PELLET           262     13      1
CATALOPY MODIF commande/calc_elem             pellet J.PELLET           541      1      6
CATALOPY MODIF commande/calc_forc_ajou        pellet J.PELLET            94      1      6
CATALOPY MODIF commande/calc_matr_ajou        pellet J.PELLET            85      1      6
CATALOPY MODIF commande/calc_precont          pellet J.PELLET           175      1      6
CATALOPY MODIF commande/dyna_line_harm        pellet J.PELLET           136      1     11
CATALOPY MODIF commande/dyna_non_line         pellet J.PELLET           402      1      6
CATALOPY MODIF commande/dyna_tran_modal       pellet J.PELLET           243      1      6
CATALOPY MODIF commande/macr_ascouf_calc      pellet J.PELLET           259      1      6
CATALOPY MODIF commande/macr_aspic_calc       pellet J.PELLET           271      1      6
CATALOPY MODIF commande/macro_matr_ajou       pellet J.PELLET           128      1      6
CATALOPY MODIF commande/meca_statique         pellet J.PELLET           116      1      6
CATALOPY MODIF commande/mode_statique         pellet J.PELLET           118      1      6
CATALOPY MODIF commande/stat_non_line         pellet J.PELLET           376      1      6
CATALOPY MODIF commande/ther_lineaire         pellet J.PELLET           124      1      6
CATALOPY MODIF commande/ther_non_line_mo      pellet J.PELLET            89      1      6
CATALOPY MODIF commande/ther_non_line         pellet J.PELLET           166      1      6
FORTRAN90 MODIF mumps/amumpc                   pellet J.PELLET           357      5      5
FORTRAN90 MODIF mumps/amumpi                   pellet J.PELLET           340      1      1
FORTRAN90 MODIF mumps/amumpm                   pellet J.PELLET           394      1      1
FORTRAN90 MODIF mumps/amumpp                   pellet J.PELLET           295      1      1
FORTRAN90 MODIF mumps/amumpr                   pellet J.PELLET           359      5      5
FORTRAN90 MODIF mumps/amumps                   pellet J.PELLET           280      1      1
FORTRAN90 MODIF mumps/amumpt                   pellet J.PELLET           452      1      1
 FORTRAN AJOUT algorith/ajlipa                pellet J.PELLET           250    250      0
 FORTRAN AJOUT assembla/parti0                pellet J.PELLET            51     51      0
 FORTRAN AJOUT modelisa/op0103                pellet J.PELLET            55     55      0
 FORTRAN MODIF algeline/creso3                pellet J.PELLET           154      5     11
 FORTRAN MODIF algeline/preres                pellet J.PELLET           255      8      2
 FORTRAN MODIF algeline/typmat                pellet J.PELLET            75      8      9
 FORTRAN MODIF algeline/vpsorn                pellet J.PELLET           368      4      2
 FORTRAN MODIF algorith/crsvmu                pellet J.PELLET           146     10    192
 FORTRAN MODIF algorith/mmligr                pellet J.PELLET           276      2      2
 FORTRAN MODIF algorith/xmligr                pellet J.PELLET           270      2      2
 FORTRAN MODIF assembla/assma1                pellet J.PELLET           187      4      6
 FORTRAN MODIF assembla/assmam                pellet J.PELLET           765     28     27
 FORTRAN MODIF assembla/assmiv                pellet J.PELLET           420     16     18
 FORTRAN MODIF assembla/assvec                pellet J.PELLET          1062     33     28
 FORTRAN MODIF assembla/op0012                pellet J.PELLET           149     11      2
 FORTRAN MODIF calculel/alrslt                pellet J.PELLET           158     13      4
 FORTRAN MODIF calculel/calcul                pellet J.PELLET           534     33     24
 FORTRAN MODIF calculel/celces                pellet J.PELLET           384      6      3
 FORTRAN MODIF calculel/cesvar                pellet J.PELLET            70      2      3
 FORTRAN MODIF calculel/meamme                pellet J.PELLET           277      8      8
 FORTRAN MODIF calculel/mecoel                pellet J.PELLET           332      2     20
 FORTRAN MODIF calculel/memax                 pellet J.PELLET           214      5      7
 FORTRAN MODIF calculel/mesomm                pellet J.PELLET           272      3      5
 FORTRAN MODIF calculel/monte1                pellet J.PELLET           146      1     13
 FORTRAN MODIF calculel/op0038                pellet J.PELLET           164      6      2
 FORTRAN MODIF elements/arlapl               meunier S.MEUNIER          144      2      1
 FORTRAN MODIF elements/arltds               meunier S.MEUNIER          102      2      1
 FORTRAN MODIF elements/arlteb               meunier S.MEUNIER          135      2      1
 FORTRAN MODIF elements/arltec               meunier S.MEUNIER           93      2      1
 FORTRAN MODIF elements/arlted               meunier S.MEUNIER          168      2      1
 FORTRAN MODIF elements/arltem               meunier S.MEUNIER          139      2      1
 FORTRAN MODIF elements/arlten               meunier S.MEUNIER           89      2      1
 FORTRAN MODIF elements/arltep               meunier S.MEUNIER          134      2      1
 FORTRAN MODIF elements/bmatf1               meunier S.MEUNIER          102      2      1
 FORTRAN MODIF elements/bmatfr               meunier S.MEUNIER           89      2      1
 FORTRAN MODIF elements/te0050                pellet J.PELLET           195      3      2
 FORTRAN MODIF elements/te0119               meunier S.MEUNIER          137      2      1
 FORTRAN MODIF from_c/mumam                   pellet J.PELLET           354     59      5
 FORTRAN MODIF modelisa/accep1                pellet J.PELLET           279      2      2
 FORTRAN MODIF modelisa/alligr                pellet J.PELLET           132      2      2
 FORTRAN MODIF modelisa/arlmol                pellet J.PELLET           178      2      2
 FORTRAN MODIF modelisa/coligr                pellet J.PELLET           237      2      2
 FORTRAN MODIF modelisa/craglc                pellet J.PELLET           137      2      2
 FORTRAN MODIF modelisa/crelgt                pellet J.PELLET           152      2      2
 FORTRAN MODIF modelisa/exlim1                pellet J.PELLET           168      8      6
 FORTRAN MODIF modelisa/lgtlgr                pellet J.PELLET           356      2      2
 FORTRAN MODIF modelisa/op0018                pellet J.PELLET           659     11      7
 FORTRAN MODIF soustruc/op0086                pellet J.PELLET           133     16      8
 FORTRAN MODIF soustruc/ssmage                pellet J.PELLET           139      3      1
 FORTRAN MODIF soustruc/ssrige                pellet J.PELLET           123      4      1
 FORTRAN MODIF supervis/execop                pellet J.PELLET           117      1      9
 FORTRAN MODIF utilitai/detrsd                pellet J.PELLET           625     10      4
 FORTRAN MODIF utilitai/dismlg                pellet J.PELLET           371     13      7
 FORTRAN MODIF utilitai/dismme                pellet J.PELLET           151     39      3
 FORTRAN MODIF utilitai/dismre                pellet J.PELLET           141      5      1
 FORTRAN MODIF utilitai/sdmpic                pellet J.PELLET            96     18     10
  PYTHON AJOUT SD/sd_partition                pellet J.PELLET            26     26      0
  PYTHON MODIF Messages/algorith16            pellet J.PELLET           486      2     12
  PYTHON MODIF Messages/calculel6             pellet J.PELLET           349     11      3
  PYTHON MODIF Messages/calculel              pellet J.PELLET           340     18      2
  PYTHON MODIF SD/sd_ligrel                   pellet J.PELLET            92     11      2


        nb unites  nb lignes  ajouts  suppr.  difference
 AJOUT :    5         420       420              +420
 MODIF :  108       27629       663     765      -102
 SUPPR :    0           0                 0        +0
 DEPLA :    0           0 
         ----      ------     ------  ------   ------
 TOTAL :  113       28049      1083     765      +318 
